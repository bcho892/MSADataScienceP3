{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Data Science Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the CIFAR-10 dataset, which is a collection of 6000 32x32 RGB images given as ararys each classified as one of 10 \"labels\" which are categories of animals or items. \n",
    "\n",
    "The categories and their corresponding labels represented by the dataset are:\n",
    "* Airplane [0]\n",
    "* Automobile [1]\n",
    "* Bird [2]\n",
    "* Cat [3]\n",
    "* Deer [4]\n",
    "* Dog [5]\n",
    "* Frog [6]\n",
    "* Horse [7]\n",
    "* Ship [8]\n",
    "* Truck [9]\n",
    "\n",
    "Our end goal is to determine the probabilty of one of these images belonging to one of the labels. I will initially arbitrarily choose to do this for label 2 **bird**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model takes in an 3d array of size (32,32,3), which is represents an 32x32 RGB image with each of its color channels. We want the output to tell us something about the probablility the image is of a specific category. Using this input:\n",
    "* we then apply preprocessing that normalises it from uint8 to a float between 0.0 and 1.0, so that each feature of the input won't have their weights significantly skewed which would likely decrease the accuracy of our model.\n",
    "* Our training targets are transformed to categorical variables so that we can carry out categorical (or binary) crossentropy, which will allow us to gauge the relative probability of an image beloning to said category.\n",
    "\n",
    "After our pre-processing, our inputs go through a series of convolution and down sampling (with MaxPooling2D); convoluting or passing the inputs through the kernel will let our neural network detect features of our image, and we are also using down sampling in between to avoid an excessive amount of features so as to avoid overfitting. \n",
    "\n",
    "Repeating the above process a few times, we then use \"dense\" layer with softmax (so we get the probabilities of each category) to get our output.\n",
    "\n",
    "Overall, out model goes from \n",
    "### (32,32,3) RGB Image &rarr; Conv2D & MaxPooling2D &rarr; (10) Probability Distribution \n",
    "\n",
    "I ended up using the adam optimiser after comparing it with SGD through hyperparameter tuning, as it yielded a higher accuracy on the first epoch. Researching I also found that for situations like this adam was the most effective in reducing loss compared to the options. For the loss, I chose Categorical Crossentropy over Binary Crossentropy because it:\n",
    "1. Gave a higher accuracy\n",
    "2. Was more relevant to the goal (gave a probability)\n",
    "3. Had less risk of overfitting (Same amount of features to predict 10 output categories vs 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Final Model Accuracy](./images/finalacc.png)\n",
    "![Final Model Loss](./images/finalloss.png)\n",
    "*Messy graph, but hovering shows that the \"correct\" line is the \"highest\" one for accuracy and the \"lowest\" one for loss*\n",
    "\n",
    "This is our graph for the model we ended up using. What we can see is:\n",
    "* The accuracy on the prediction model seems pleatau around 8 epochs, which is where the training automatically stopped due to the loss not decreasing anymore (i.e a minimum was found, don't know if it is local or not, but looking at the previous run, probably.).\n",
    "  * If we were to go further, our model would overfit, i.e get better at predicting for training data, but worse on \"random\", new data.\n",
    "* Overall, we managed to achieve an accuracy of 0.76 for the testing data.\n",
    "* After testing with some random images which I converted to 32x32 RGB I found that it seemed to give satisfactory predictions.\n",
    "\n",
    "One weakness of this model is when if the data does not contain anything from the given categories, as the results are a probablilty distribution so it will give a (wrong) prediction that the image is likely something from the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, making use of hyperparameter tuning and researching how deep learning for image recognition was done, I was able to come up with a model that has a decent amount of accuracy and able to predict the probability that a certain item/animal is present/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009.\n",
    "2. [Various Optimization Algorithms For Training Neural Network](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6), Sanket Doshi, 2019\n",
    "3. [How to Develop a CNN From Scratch for CIFAR-10 Photo Classification](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/), Jason Brownlee, 2019\n",
    "4. [CIFAR-10 Image Classification in TensorFlow](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c), Park Chansung, 2018\n",
    "5. [Hyperparameter Tuning with the HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams), TensorFlow, 2022\n",
    "6. [What is the Softmax Function?](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer), Thomas Wood, DeepAI\n",
    "7. [Conv2d: Finally Understand What Happens in the Forward Pass](https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148), Alex Thevenot, 2020\n",
    "8. [Why Data Should be Normalized before Training a Neural Network](https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d) Timo St√∂ttner, 2019\n",
    "9. [Categorical crossentropy](https://peltarion.com/knowledge-center/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy), peltarion.com\n",
    "10. [Kernal (image processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing)), Wikipedia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
